{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\garik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\garik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\garik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Found 42500 images belonging to 13 classes.\n",
      "Number of images found: 42500\n",
      "Number of classes found: 13\n",
      "   2/1329 [..............................] - ETA: 39:19"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24/1329 [..............................] - ETA: 48:20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:3182: DecompressionBombWarning: Image size (96714256 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110/1329 [========================>.....] - ETA: 6:18"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329/1329 [==============================] - 2277s 2s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Directory where your images are located\n",
    "dataset_dir = r\"D:\\SEM-4\\PROJECTS\\ML\\Archive\"\n",
    "\n",
    "# Initialize VGG16 model, this time including the top layers\n",
    "base_model = VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "# However, instead of using the model as is, create a new model that outputs the features from the penultimate layer\n",
    "# The penultimate layer is the one before the final classification layer, and it has 4096 features\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
    "\n",
    "# Create an instance of the ImageDataGenerator for loading images\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Create a data generator for reading images from directories\n",
    "generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,  # Adjust based on your GPU memory\n",
    "    class_mode='sparse',  # 'sparse' yields integer labels\n",
    "    shuffle=False  # Important for keeping labels in order\n",
    ")\n",
    "\n",
    "# Number of images and labels\n",
    "num_images = generator.samples\n",
    "print(\"Number of images found:\", num_images)\n",
    "num_classes = generator.num_classes\n",
    "print(\"Number of classes found:\", num_classes)\n",
    "\n",
    "# Extract features\n",
    "features = model.predict(generator, steps=np.ceil(num_images/32), verbose=1)\n",
    "\n",
    "# Get the labels (ensure they are in the same order as the images)\n",
    "labels = generator.classes\n",
    "\n",
    "# Saving features and labels to .npy files\n",
    "np.save('features_4096.npy', features)\n",
    "np.save('labels_4096.npy', labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42500, 4096)\n",
      "(42500,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "features = np.load(r'D:\\SEM-4\\ML\\CODES\\Machine-Learning\\features_4096.npy')\n",
    "size = np.shape(features)\n",
    "print(size)\n",
    "\n",
    "labels = np.load(r'D:\\SEM-4\\ML\\CODES\\Machine-Learning\\labels_4096.npy')\n",
    "size1 = np.shape(labels)\n",
    "print(size1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_neighbors': 15}\n",
      "Best accuracy found:  0.5213823529411765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.38      0.45       276\n",
      "           1       0.77      0.46      0.58       646\n",
      "           2       0.39      0.66      0.49      1081\n",
      "           3       0.63      0.45      0.53       506\n",
      "           4       0.69      0.63      0.66       448\n",
      "           5       0.60      0.61      0.60       594\n",
      "           6       0.94      0.31      0.47       269\n",
      "           7       0.48      0.61      0.54      1088\n",
      "           8       0.60      0.57      0.58      1204\n",
      "           9       0.33      0.54      0.41       498\n",
      "          10       0.61      0.49      0.55      1352\n",
      "          11       0.42      0.12      0.18       318\n",
      "          12       0.88      0.65      0.75       220\n",
      "\n",
      "    accuracy                           0.53      8500\n",
      "   macro avg       0.61      0.50      0.52      8500\n",
      "weighted avg       0.57      0.53      0.53      8500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming features and labels are already loaded from the .npy files\n",
    "features = np.load('D://SEM-4//ML//CODES\\Machine-Learning//features_4096.npy')\n",
    "labels = np.load('D:\\SEM-4\\ML\\CODES\\Machine-Learning\\labels_4096.npy')\n",
    "\n",
    "# Reshape features for kNN\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid: number of neighbors\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# Initialize a kNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit it to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best accuracy found: \", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set with the best parameters\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1305\n",
      "Class 1: 3035\n",
      "Class 2: 5312\n",
      "Class 3: 2607\n",
      "Class 4: 2235\n",
      "Class 5: 3115\n",
      "Class 6: 1324\n",
      "Class 7: 5373\n",
      "Class 8: 6192\n",
      "Class 9: 2521\n",
      "Class 10: 6813\n",
      "Class 11: 1510\n",
      "Class 12: 1158\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load your labels if not already loaded\n",
    "labels = np.load('D:\\SEM-4\\ML\\CODES\\Machine-Learning\\labels_4096.npy')\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "# Printing each class with its count on a new line\n",
    "for class_label, count in class_distribution.items():\n",
    "    print(f\"Class {class_label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Performance Metrics:\n",
      "Accuracy: 0.59\n",
      "Precision: 0.59\n",
      "Recall: 0.59\n",
      "F1 Score: 0.58\n",
      "Confusion Matrix:\n",
      "[[ 158    6   36    5    8   16    0   38   51   20   64    2    4]\n",
      " [   6  587   11   51   63   17   14   36   66    5   61    9   10]\n",
      " [  10   14  942   13    2   28    9  106  244   73  180    7    0]\n",
      " [   1   46   24  442   30   14   19   49   60    2   47   17    9]\n",
      " [   2   58    2   24  497    5    8   21   26    2   23    5    2]\n",
      " [  17   28   85    5    3  516    4   14   84   52   89    3    1]\n",
      " [   0   23   12   32   11    6  215   22   39    0   21    6    3]\n",
      " [   9   16  112   28   12   20    6 1009   79   29  265    7    4]\n",
      " [   6   39  231   35   14   36    9   58 1306   26   61   11   30]\n",
      " [  16    8  148    5    2   28    1   39   74  302  113    1    1]\n",
      " [  24   36  150   26    4   54   10  247  179   69 1220   18    8]\n",
      " [   2   19   36   42   16   14    9  112   86    3   58   73    7]\n",
      " [   0   13    2   12    1    0    2    1   54    0    6    2  241]]\n",
      "\n",
      "\n",
      "XGBoost Performance Metrics:\n",
      "Accuracy: 0.58\n",
      "Precision: 0.59\n",
      "Recall: 0.58\n",
      "F1 Score: 0.58\n",
      "Confusion Matrix:\n",
      "[[ 136    0   49    3    3   15    0   44   53   15   84    3    3]\n",
      " [   2  565   17   43   48   15   12   47   72    5   93    8    9]\n",
      " [   8   11  971   14    3   30    4  102  217   73  187    7    1]\n",
      " [   0   44   27  433   16   12   11   45   84    1   67   16    4]\n",
      " [   3   67    4   21  487    8    3   22   22    0   29    8    1]\n",
      " [  11   17   86    6    1  515    1   24   95   43  100    2    0]\n",
      " [   0   33   12   28    9    5  189   26   49    0   31    7    1]\n",
      " [   6    9  134   19    5   21    5 1000   83   28  273   10    3]\n",
      " [   3   25  258   29   13   24    3   64 1289   20  106   11   17]\n",
      " [  10    7  163    4    3   38    0   37   75  289  110    2    0]\n",
      " [  17   23  161   13    7   37    3  262  190   66 1248   10    8]\n",
      " [   5   16   35   32    7   15    4  104   95    3   91   65    5]\n",
      " [   0   10    3   14    2    1    1    4   61    0    7    2  229]]\n",
      "\n",
      "\n",
      "SVM Performance Metrics:\n",
      "Accuracy: 0.64\n",
      "Precision: 0.64\n",
      "Recall: 0.64\n",
      "F1 Score: 0.63\n",
      "Confusion Matrix:\n",
      "[[ 158    6   41    5    6   12    0   47   45   26   56    4    2]\n",
      " [   3  636   16   51   42    9   10   38   50    4   60    9    8]\n",
      " [  10   14 1039    7    3   15    5   97  208   62  167    1    0]\n",
      " [   1   37   24  498   20   11   14   47   55    1   37   11    4]\n",
      " [   2   56    4   26  516    2    6   21   17    0   22    3    0]\n",
      " [  15   23   76    7    1  533    1   18   78   67   78    3    1]\n",
      " [   0   18   11   31    8    3  244   20   32    1   17    4    1]\n",
      " [  12   12  108   23    9   15    5 1095   64   23  221    7    2]\n",
      " [   3   31  212   28   14   24    4   55 1387   19   58    8   19]\n",
      " [  13    5  136    3    2   18    0   33   69  353  105    1    0]\n",
      " [  18   29  128   30    6   28    7  246  161   75 1302   12    3]\n",
      " [   4   21   32   42    9    7    9  109   85    2   63   87    7]\n",
      " [   0   14    4   12    3    0    0    2   41    0    4    0  254]]\n",
      "\n",
      "\n",
      "Random Forest Performance Metrics:\n",
      "Accuracy: 0.52\n",
      "Precision: 0.57\n",
      "Recall: 0.52\n",
      "F1 Score: 0.50\n",
      "Confusion Matrix:\n",
      "[[  73    4   85    1    4    4    0   42  100   29   65    0    1]\n",
      " [   0  544   21   30   58   10    0   39  125    3  101    0    5]\n",
      " [   3   12  923    5    4   24    1  107  279   42  226    0    2]\n",
      " [   0   71   36  300   35   16    1   60  159    1   79    0    2]\n",
      " [   0   68    6   27  452    8    0   32   39    2   39    0    2]\n",
      " [   7   36  113    1    3  442    0   21  130   43  105    0    0]\n",
      " [   0   31   29   40   15    3   84   46   96    0   44    0    2]\n",
      " [   0   13  153    9    9   20    0  933  115   31  312    0    1]\n",
      " [   0   51  283   29   13   30    0   59 1271   14  103    0    9]\n",
      " [   4    9  237    0    1   15    0   34  115  206  116    0    1]\n",
      " [   5   34  221    8   10   36    1  230  254   57 1187    0    2]\n",
      " [   2   21   40   24    6    9    0  119  136    1  117    1    1]\n",
      " [   0   21    1   11    3    0    0    2  104    0    6    0  186]]\n",
      "\n",
      "\n",
      "AdaBoost Performance Metrics:\n",
      "Accuracy: 0.29\n",
      "Precision: 0.28\n",
      "Recall: 0.29\n",
      "F1 Score: 0.27\n",
      "Confusion Matrix:\n",
      "[[  5  13 189   4  11  11   2  50  71  24  22   0   6]\n",
      " [  2 323  76  42 147  58  22  38 109   3  56   0  60]\n",
      " [  7  61 785  25  28  50  10 172 231  75 161   0  23]\n",
      " [  2  85  63 135 101  34  36  65 124   8  49   0  58]\n",
      " [  0 127  17  60 291  16  17  52  37   1  29   0  28]\n",
      " [  4 104 186  17  58 168   6  56 130  28 125   0  19]\n",
      " [  0  40  32  33  48  16  41  41  68   5  36   1  29]\n",
      " [ 11  49 340  40  48  48  11 561 125  31 323   1   8]\n",
      " [  5 117 482  61  62  74  23 139 566  69  98   2 164]\n",
      " [  4  11 381   9  18  18   9  60  95  73  51   0   9]\n",
      " [  4  98 438  31  61  81  14 553 212  45 485   2  21]\n",
      " [  4  25  98  25  33  21   9 108  83   8  47   2  14]\n",
      " [  0   8   1  11   6   8   1   1  43   2  17   1 235]]\n",
      "\n",
      "\n",
      "Decision Tree Performance Metrics:\n",
      "Accuracy: 0.30\n",
      "Precision: 0.30\n",
      "Recall: 0.30\n",
      "F1 Score: 0.30\n",
      "Confusion Matrix:\n",
      "[[ 54  17  48  14  16  28   7  47  64  36  58  16   3]\n",
      " [ 10 299  41  97 105  60  34  46  87  22  85  25  25]\n",
      " [ 48  38 451  55  21  99  22 187 255 152 238  44  18]\n",
      " [ 18  64  60 190  49  23  33  70  98  27  61  42  25]\n",
      " [  3  96  22  55 277  29  20  43  48   9  43  16  14]\n",
      " [ 31  44 112  28  15 287  19  56 100  62 121  20   6]\n",
      " [ 12  31  31  41  19  18  76  41  46   9  40  14  12]\n",
      " [ 33  56 160  65  29  73  39 569 121  66 309  63  13]\n",
      " [ 68  96 257  92  42 100  39 121 650 105 164  72  56]\n",
      " [ 35   9 169  16   6  66  11  66  87 156  92  13  12]\n",
      " [ 49  79 224  86  37 121  43 294 177 115 736  67  17]\n",
      " [ 13  26  50  44  19  28  17  84  66  17  64  37  12]\n",
      " [  4  24  17  30  14  19   8  17  57   4  20  14 106]]\n",
      "\n",
      "\n",
      "Naive Bayes Performance Metrics:\n",
      "Accuracy: 0.39\n",
      "Precision: 0.44\n",
      "Recall: 0.39\n",
      "F1 Score: 0.38\n",
      "Confusion Matrix:\n",
      "[[225   9  20   1  10   9   3  21  21  62   5  15   7]\n",
      " [ 34 341  10  29 221  50  41  39  40   6  30  28  67]\n",
      " [116  19 547  25  49  69  36 159 103 328 101  37  39]\n",
      " [ 33  46  29 217 117  28  48  35  40  25  23  52  67]\n",
      " [  4  29   0  22 506   8  31  20   4  10  10  16  15]\n",
      " [ 89  32  41   2  82 323  18  27  27 157  36  11  56]\n",
      " [ 16  14  11  21  41   8 178  24  19   7  18  10  23]\n",
      " [ 91  18 117  29  27  64  43 863  28 165  84  41  26]\n",
      " [160  61 250  55  70  90  31  96 480 203  28 104 234]\n",
      " [ 70   6  91   6  13  16   6  53  27 376  35  19  20]\n",
      " [134  57 117  23  59 190  25 464  96 197 533  65  85]\n",
      " [ 41  19  23  25  27  17  16 110  28  28  21  93  29]\n",
      " [  0   4   2  11   9   2   9   1  13   3   1   3 276]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths to the features and labels files\n",
    "features_path = 'D://SEM-4//ML//CODES//Machine-Learning//features_4096.npy'\n",
    "labels_path = 'D:\\SEM-4\\ML\\CODES\\Machine-Learning\\labels_4096.npy'\n",
    "\n",
    "# Load features and labels\n",
    "features = np.load(features_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "# Reshape features from 4D (n_samples, height, width, channels) to 2D (n_samples, height*width*channels)\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Predict the responses for the test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # Store results\n",
    "    results[name] = (accuracy, precision, recall, f1, conf_matrix)\n",
    "\n",
    "# Print all results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name} Performance Metrics:\")\n",
    "    print(f\"Accuracy: {metrics[0]:.2f}\")\n",
    "    print(f\"Precision: {metrics[1]:.2f}\")\n",
    "    print(f\"Recall: {metrics[2]:.2f}\")\n",
    "    print(f\"F1 Score: {metrics[3]:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics[4])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA to explain 99% variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components selected to explain at least 99% of the variance: 3300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paths to the features and labels files\n",
    "features_path = 'D://SEM-4//ML//CODES//Machine-Learning//features_4096.npy'\n",
    "labels_path = 'D:\\SEM-4\\ML\\CODES\\Machine-Learning\\labels_4096.npy'# Load features and labels\n",
    "features = np.load(features_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "# Reshape features from 4D (n_samples, height, width, channels) to 2D (n_samples, height*width*channels)\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Applying PCA to capture 99% of the variance\n",
    "pca = PCA(0.99)\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# Saving the reduced features to a new file\n",
    "reduced_features_path = 'D://SEM-4//ML//CODES//Machine-Learning//Reduced//reduced_features.npy'\n",
    "np.save(reduced_features_path, features_pca)\n",
    "\n",
    "# Optionally, save the labels if you need to keep them aligned with the reduced features for later use\n",
    "reduced_labels_path = 'D://SEM-4//ML//CODES//Machine-Learning//Reduced//reduced_labels.npy'\n",
    "np.save(reduced_labels_path, labels)\n",
    "\n",
    "# Number of components selected\n",
    "n_components = pca.n_components_\n",
    "print(f\"Number of principal components selected to explain at least 99% of the variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths to the features and labels files\n",
    "features_path = 'D://SEM-4//ML//CODES//Machine-Learning//Reduced//reduced_features.npy'\n",
    "labels_path = 'D://SEM-4//ML//CODES//Machine-Learning//Reduced//reduced_labels.npy'\n",
    "\n",
    "# Load features and labels\n",
    "features = np.load(features_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "# Reshape features from 4D (n_samples, height, width, channels) to 2D (n_samples, height*width*channels)\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Predict the responses for the test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # Store results\n",
    "    results[name] = (accuracy, precision, recall, f1, conf_matrix)\n",
    "\n",
    "# Print all results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name} Performance Metrics:\")\n",
    "    print(f\"Accuracy: {metrics[0]:.2f}\")\n",
    "    print(f\"Precision: {metrics[1]:.2f}\")\n",
    "    print(f\"Recall: {metrics[2]:.2f}\")\n",
    "    print(f\"F1 Score: {metrics[3]:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics[4])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
