{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing scikit-learn: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing XGBoost: {e}\")\n",
    "    # Optionally continue without XGBoost if it's not crucial\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing CatBoost: {e}\")\n",
    "    # Optionally continue without CatBoost if it's not crucial\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing imbalanced-learn: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load features and labels\n",
    "features_path = 'D://SEM-//\\ML//CODES//Machine-Learning//Lab04//extracted_features.npy'\n",
    "labels_path = 'D://SEM-4//ML//CODES//Machine-Learning//Lab04//labels.npy'\n",
    "\n",
    "features = np.load(features_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "# Reshape features from 4D (n_samples, height, width, channels) to 2D (n_samples, height*width*channels)\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 0.1, 0.01],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 10]\n",
    "}\n",
    "\n",
    "param_grid_nb = {}  # GaussianNB doesn't have many parameters to tune\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [6, 10],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "param_grid_catboost = {\n",
    "    'iterations': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'depth': [4, 6, 10]\n",
    "}\n",
    "\n",
    "# Setup cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"CatBoost\": GridSearchCV(CatBoostClassifier(verbose=0), param_grid_catboost, cv=cv_strategy, scoring='accuracy') if 'CatBoostClassifier' in globals() else None,\n",
    "    \"XGBoost\": GridSearchCV(XGBClassifier(), param_grid_xgb, cv=cv_strategy, scoring='accuracy') if 'XGBClassifier' in globals() else None,\n",
    "    \"SVM\": GridSearchCV(SVC(), param_grid_svm, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"Random Forest\": GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"AdaBoost\": GridSearchCV(AdaBoostClassifier(), param_grid_adaboost, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"Decision Tree\": GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"Naive Bayes\": GaussianNB() if not param_grid_nb else GridSearchCV(GaussianNB(), param_grid_nb, cv=cv_strategy, scoring='accuracy')\n",
    "}\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    if clf is None:\n",
    "        print(f\"{name} is not available due to an import error.\")\n",
    "        continue\n",
    "    # Start timer for training\n",
    "    start_time_train = time.time()\n",
    "    \n",
    "    # Train the classifier with SMOTE-applied training data\n",
    "    clf.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # End timer for training\n",
    "    end_time_train = time.time()\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = end_time_train - start_time_train\n",
    "    \n",
    "    # Start timer for prediction\n",
    "    start_time_pred = time.time()\n",
    "    \n",
    "    # Predict the responses for the test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # End timer for prediction\n",
    "    end_time_pred = time.time()\n",
    "    \n",
    "    # Calculate prediction time\n",
    "    prediction_time = end_time_pred - start_time_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print best parameters (if clf uses GridSearchCV)\n",
    "    best_params = clf.best_params_ if isinstance(clf, GridSearchCV) else \"N/A\"\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = (accuracy, precision, recall, f1, conf_matrix, training_time, prediction_time, best_params)\n",
    "\n",
    "# Print all results\n",
    "for name, metrics in results.items():\n",
    "    if metrics:\n",
    "        print(f\"{name} Performance Metrics:\")\n",
    "        print(f\"Accuracy: {metrics[0]:.2f}\")\n",
    "        print(f\"Precision: {metrics[1]:.2f}\")\n",
    "        print(f\"Recall: {metrics[2]:.2f}\")\n",
    "        print(f\"F1 Score: {metrics[3]:.2f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(metrics[4])\n",
    "        print(f\"Training Time: {metrics[5]:.4f} seconds\")\n",
    "        print(f\"Prediction Time: {metrics[6]:.4f} seconds\")\n",
    "        print(f\"Best Parameters: {metrics[7]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing scikit-learn: {e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing XGBoost: {e}\")\n",
    "    # Optionally continue without XGBoost if it's not crucial\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing CatBoost: {e}\")\n",
    "    # Optionally continue without CatBoost if it's not crucial\n",
    "\n",
    "# Load features and labels\n",
    "features_path = 'D://SEM-4//ML//CODES//Machine-Learning//Lab04//extracted_features.npy'\n",
    "labels_path = 'D://SEM-4//ML//CODES//Machine-Learning//Lab04//labels.npy'\n",
    "\n",
    "features = np.load(features_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "# Reshape features from 4D (n_samples, height, width, channels) to 2D (n_samples, height*width*channels)\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 0.1, 0.01],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 10]\n",
    "}\n",
    "\n",
    "param_grid_nb = {}  # GaussianNB doesn't have many parameters to tune\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [6, 10],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "param_grid_catboost = {\n",
    "    'iterations': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'depth': [4, 6, 10]\n",
    "}\n",
    "\n",
    "# Setup cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"CatBoost\": GridSearchCV(CatBoostClassifier(verbose=0), param_grid_catboost, cv=cv_strategy, scoring='accuracy') if 'CatBoostClassifier' in globals() else None,\n",
    "    \"XGBoost\": GridSearchCV(XGBClassifier(), param_grid_xgb, cv=cv_strategy, scoring='accuracy') if 'XGBClassifier' in globals() else None,\n",
    "    \"SVM\": GridSearchCV(SVC(), param_grid_svm, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"Random Forest\": GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"AdaBoost\": GridSearchCV(AdaBoostClassifier(), param_grid_adaboost, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"Decision Tree\": GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=cv_strategy, scoring='accuracy'),\n",
    "    \"Naive Bayes\": GaussianNB() if not param_grid_nb else GridSearchCV(GaussianNB(), param_grid_nb, cv=cv_strategy, scoring='accuracy')\n",
    "}\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    if clf is None:\n",
    "        print(f\"{name} is not available due to an import error.\")\n",
    "        continue\n",
    "    # Start timer for training\n",
    "    start_time_train = time.time()\n",
    "    \n",
    "    # Train the classifier with training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # End timer for training\n",
    "    end_time_train = time.time()\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = end_time_train - start_time_train\n",
    "    \n",
    "    # Start timer for prediction\n",
    "    start_time_pred = time.time()\n",
    "    \n",
    "    # Predict the responses for the test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # End timer for prediction\n",
    "    end_time_pred = time.time()\n",
    "    \n",
    "    # Calculate prediction time\n",
    "    prediction_time = end_time_pred - start_time_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print best parameters (if clf uses GridSearchCV)\n",
    "    best_params = clf.best_params_ if isinstance(clf, GridSearchCV) else \"N/A\"\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = (accuracy, precision, recall, f1, conf_matrix, training_time, prediction_time, best_params)\n",
    "\n",
    "# Print all results\n",
    "for name, metrics in results.items():\n",
    "    if metrics:\n",
    "        print(f\"{name} Performance Metrics:\")\n",
    "        print(f\"Accuracy: {metrics[0]:.2f}\")\n",
    "        print(f\"Precision: {metrics[1]:.2f}\")\n",
    "        print(f\"Recall: {metrics[2]:.2f}\")\n",
    "        print(f\"F1 Score: {metrics[3]:.2f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(metrics[4])\n",
    "        print(f\"Training Time: {metrics[5]:.4f} seconds\")\n",
    "        print(f\"Prediction Time: {metrics[6]:.4f} seconds\")\n",
    "        print(f\"Best Parameters: {metrics[7]}\")\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
